# ═══════════════════════════════════════════════════════════════
# CHARACTER CHAT - ENVIRONMENT CONFIGURATION
# ═══════════════════════════════════════════════════════════════

# ─────────────────────────────────────────────────────────────────
# AUTHENTICATION (Required for Google Sign-In)
# ─────────────────────────────────────────────────────────────────
# Get these from: https://console.cloud.google.com/apis/credentials
GOOGLE_CLIENT_ID=your_google_client_id_here
GOOGLE_CLIENT_SECRET=your_google_client_secret_here
GOOGLE_REDIRECT_URI=http://localhost:8000/auth/google/callback

# JWT Configuration
JWT_SECRET_KEY=your-super-secret-key-change-in-production-min-32-chars
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=15
JWT_REFRESH_TOKEN_EXPIRE_DAYS=7

# Cookie settings (set AUTH_COOKIE_SECURE=false for local development)
AUTH_COOKIE_SECURE=false
AUTH_COOKIE_SAMESITE=lax

# ─────────────────────────────────────────────────────────────────
# AI/LLM CONFIGURATION
# ─────────────────────────────────────────────────────────────────
# API Keys
LLM_API_KEY=your_openai_api_key_here
EMBEDDING_API_KEY=your_openai_api_key_here

# Database Configuration
MONGODB_URI=mongodb://localhost:27017/character_chat
MONGODB_DB=character_chat
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password

# Qdrant
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Redis/Job Queue
REDIS_HOST=localhost
REDIS_PORT=6379

# ─────────────────────────────────────────────────────────────────
# LLM PROVIDER CONFIGURATION
# ─────────────────────────────────────────────────────────────────
# Provider options: "openai", "vllm", "ollama"
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o-mini
LLM_TEMPERATURE=0.7
LLM_BASE_URL=https://api.openai.com/v1
LLM_MAX_TOKENS=2048
LLM_TIMEOUT=60

# For vLLM (self-hosted):
# LLM_PROVIDER=vllm
# LLM_MODEL=meta-llama/Llama-3.1-8B-Instruct
# LLM_BASE_URL=http://localhost:8000/v1

# For Ollama (local development):
# LLM_PROVIDER=ollama
# LLM_MODEL=llama3.1:8b
# LLM_BASE_URL=http://localhost:11434

# vLLM Server Settings (when self-hosting)
VLLM_TENSOR_PARALLEL=1
VLLM_MAX_MODEL_LEN=8192
VLLM_GPU_MEMORY=0.90

# HuggingFace token (for gated models like Llama)
HUGGING_FACE_HUB_TOKEN=your_hf_token_here

# ─────────────────────────────────────────────────────────────────
# EMBEDDING PROVIDER CONFIGURATION  
# ─────────────────────────────────────────────────────────────────
# Provider options: "openai", "vllm", "local"
EMBEDDING_PROVIDER=openai
EMBEDDING_MODEL=text-embedding-3-large
EMBEDDING_BASE_URL=https://api.openai.com/v1

# For local embeddings:
# EMBEDDING_PROVIDER=local
# EMBEDDING_MODEL=nomic-embed-text

# --- Worker & Processing Configuration ---
# Maximum number of parallel requests to send to the LLM provider
MAX_CONCURRENT_LLM_CALLS=10
# Maximum file size in MB for uploads
MAX_FILE_SIZE_MB=50
# Text chunking parameters
CHUNK_SIZE=1400
CHUNK_OVERLAP=200
# Number of chunks to retrieve for RAG context
RAG_RETRIEVAL_K=4
# Override embedding dimension (auto-detected by default)
VECTOR_SIZE=1536

# Storage
UPLOAD_ROOT=uploads

# ─────────────────────────────────────────────────────────────────
# TEST UTILITIES
# ─────────────────────────────────────────────────────────────────
API_BASE_URL=http://localhost:8000
TEST_USER_ID=demo-user